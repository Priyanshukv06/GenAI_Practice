{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPerson Eg: Krish C Naik\\nPlace Or Location Eg: India\\nDate Eg: September,24-09-1989\\nTime  Eg: 4:30pm\\nMoney Eg: 1 million dollar\\nOrganization Eg: iNeuron Private Limited\\nPercent Eg: 20%, twenty percent\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structures.\"\n",
    "\"\"\"\n",
    "Person Eg: Krish C Naik\n",
    "Place Or Location Eg: India\n",
    "Date Eg: September,24-09-1989\n",
    "Time  Eg: 4:30pm\n",
    "Money Eg: 1 million dollar\n",
    "Organization Eg: iNeuron Private Limited\n",
    "Percent Eg: 20%, twenty percent\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words=nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements=nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\priya/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\priya/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\priya/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\priya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\priya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\priya/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\priya/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\priya/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Define your text\n",
    "text = \"\"\"\n",
    "In a world where technology advances at a rapid pace, it is essential for individuals to stay updated with the latest developments \n",
    "in order to remain competitive in their respective fields. Continuous learning and adaptation are key components in achieving success \n",
    "and maintaining relevance in an ever-evolving landscape. Moreover, effective communication skills, both verbal and written, play a \n",
    "crucial role in conveying ideas clearly and persuasively. By embracing these principles, professionals can navigate the complexities \n",
    "of the modern workplace and drive innovation in their industries.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text\n",
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in set(stopwords.words('english'))]\n",
    "    \n",
    "    # Perform POS tagging\n",
    "    pos_tagged = pos_tag(filtered_words)\n",
    "    \n",
    "    # Perform Named Entity Recognition\n",
    "    named_entities = ne_chunk(pos_tagged)\n",
    "    \n",
    "    # Print named entities\n",
    "    for chunk in named_entities:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "\n",
    "# Optional: Display named entity chunks in a more readable format\n",
    "def print_named_entities(named_entities):\n",
    "    for chunk in named_entities:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(f\"{chunk.label()}: {' '.join(c[0] for c in chunk)}\")\n",
    "\n",
    "# Process the text and print named entities\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    filtered_words = [word for word in words if word.lower() not in set(stopwords.words('english'))]\n",
    "    pos_tagged = pos_tag(filtered_words)\n",
    "    named_entities = ne_chunk(pos_tagged)\n",
    "    print_named_entities(named_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\priya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\priya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\priya/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\priya/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\priya/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your text\n",
    "text = \"\"\"\n",
    "In a world where technology advances at a rapid pace, it is essential for individuals to stay updated with the latest developments \n",
    "in order to remain competitive in their respective fields. Continuous learning and adaptation are key components in achieving success \n",
    "and maintaining relevance in an ever-evolving landscape. Moreover, effective communication skills, both verbal and written, play a \n",
    "crucial role in conveying ideas clearly and persuasively. By embracing these principles, professionals can navigate the complexities \n",
    "of the modern workplace and drive innovation in their industries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text\n",
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in set(stopwords.words('english'))]\n",
    "    \n",
    "    # Perform POS tagging\n",
    "    pos_tagged = pos_tag(filtered_words)\n",
    "    \n",
    "    # Perform Named Entity Recognition\n",
    "    named_entities = ne_chunk(pos_tagged)\n",
    "    \n",
    "    # Print named entities\n",
    "    for chunk in named_entities:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Display named entity chunks in a more readable format\n",
    "def print_named_entities(named_entities):\n",
    "    for chunk in named_entities:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(f\"{chunk.label()}: {' '.join(c[0] for c in chunk)}\")\n",
    "\n",
    "# Process the text and print named entities\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    filtered_words = [word for word in words if word.lower() not in set(stopwords.words('english'))]\n",
    "    pos_tagged = pos_tag(filtered_words)\n",
    "    named_entities = ne_chunk(pos_tagged)\n",
    "    print_named_entities(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
